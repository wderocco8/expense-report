Table expense_report_jobs {
  id              bigint [pk, increment]
  title           text
  user_id         bigint
  title           varchar(255)
  status          varchar(20)
  total_files     int
  processed_files int
  created_at      timestamp [default: `now()`]
  updated_at      timestamp [default: `now()`]

  Note: 'Represents a batch upload job'
}

Table receipt_files {
  id                  bigint [pk, increment]
  job_id              bigint [ref: > expense_report_jobs.id]
  s3_url              text
  original_filename   varchar(255)
  status              varchar(20) // pending, processing, done, error
  error_message       text
  // ocr_text        text       // optional: this would exist if we switched to an OCR -> LLM workflow (right now we are directly calling gpt)
  created_at          timestamp [default: `now()`]
  processed_at        timestamp
}

Table extracted_expenses {
  id               bigint [pk, increment]
  receipt_id       bigint [ref: > receipt_files.id]
  job_id           bigint [ref: > expense_report_jobs.id]
  merchant         varchar(255)
  amount           numeric(10, 2)
  category         varchar(100)
  date             date
  raw_json         jsonb
  model_version    varchar(100)  // track which model was used
  created_at       timestamp [default: `now()`]
  processed_at     timestamp
}

Enum status {
  pending
  processing
  done
  error
}


// status mappings
// pending: job created, no files
// processing: at least one file pending or processing
// complete: all files have status=done
// failed: at least one file failed and user didn't retry